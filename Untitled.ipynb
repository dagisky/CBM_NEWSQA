{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06fde04de236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgmtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMultiHeadRln\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/PhD/CODES/Conceptual-Base-Model/Utils/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "# from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from time import gmtime, strftime\n",
    "from MultiHeadRln import EncoderModel\n",
    "from Utils.utils import Logger\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from utils.vis import visualize\n",
    "\n",
    "from utils import load_data, convert_examples_to_features\n",
    "from pytorch_transformers import BertTokenizer, BertModel #*\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_list = [4, 6, 7] # List of GPU cards to run on [4, 6, 7]\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, mask, seqlens, mloss, optim, args): \n",
    "    pred, std = model(x, mask, seqlens) \n",
    "    log_softmax = nn.LogSoftmax()\n",
    "    optim.zero_grad()  \n",
    "    loss = mloss(log_softmax(pred), y)\n",
    "   \n",
    "    std = torch.sum(std)\n",
    "\n",
    "    loss = loss + args.std_alpha*std\n",
    "    \n",
    "    loss.backward()\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--training', default=True)\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--std-alpha', default=0.003, type=float)\n",
    "    parser.add_argument('--test-split', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=10000, type=int)\n",
    "    parser.add_argument('--epoch_start', default=0, type=int)\n",
    "    parser.add_argument('--exp-decay-rate', default=0.99, type=float)\n",
    "    parser.add_argument('--use_cuda', default=True)\n",
    "    parser.add_argument('--hidden-size', default=100, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.05, type=float)\n",
    "    parser.add_argument('--print-freq', default=250, type=int)\n",
    "    parser.add_argument('--train-batch-size', default=60, type=int)\n",
    "    parser.add_argument('--dev-batch-size', default=100, type=int)\n",
    "    parser.add_argument('--model-config', default='rnn_config.ini')\n",
    "    parser.add_argument('--config', default='config.json')\n",
    "    parser.add_argument('--clip', type=float, default=1, help='gradient clipping')\n",
    "    parser.add_argument('--model-name', default='MultiHeadRln')\n",
    "    parser.add_argument('--word-dim', default=100, type=int)\n",
    "    parser.add_argument('--resume', default='MultiHeadRln_18 _300.model', type=str, metavar='PATH', help='path saved params')\n",
    "    parser.add_argument(\"--output_dir\", default='./checkpoints/', type=str, \n",
    "                        help=\"The output directory where the model checkpoints will be written.\")\n",
    "    # [/mnt/disk/dagi/BiDAF_multiHead/checkpoints/], [./checkpoints/]\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(f\"cuda:{str(gpu_list[0])}\" if args.use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    print('loading BABI data...')\n",
    "    setattr(args, 'dataset', 'Task'+args.task+'_text_10K') #\n",
    "    setattr(args, 'device', device)\n",
    "    setattr(args, 'model_time', strftime('%H_%M_%S', gmtime()))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    data = load_data()\n",
    "\n",
    "    with open(args.config) as config_file: \n",
    "        hyp = json.load(config_file)['hyperparams']  \n",
    "   \n",
    " \n",
    "    model = EncoderModel(args, hyp)\n",
    "    model = nn.DataParallel(model, device_ids=gpu_list)\n",
    "\n",
    "    model_loss = nn.NLLLoss()\n",
    "    optimizer  = torch.optim.SGD(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_start_pos = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "    all_end_pos = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_start_pos, all_end_pos, all_segment_ids, all_example_index)\n",
    "\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "\n",
    "    if os.path.exists(f'model/{args.resume}'):\n",
    "        print(f'loading ..........model/{args.resume}')\n",
    "        model.load_state_dict(torch.load(f'model/{args.resume}', map_location=args.device)) #  map_location\n",
    "    # loss, acc, total = 0, 0, 0\n",
    "    for e in range(args.epoch_start, args.epoch):        \n",
    "        loss, acc, total = 0.0, 0.0, 0.0\n",
    "        for i, batch in enumerate(eval_dataloader): \n",
    "\n",
    "            batch_loss, out = train(model, batch[0], batch.answer, batch[1], batch.dialog[1], model_loss, optimizer, args)            \n",
    "            batch_pred = torch.argmax(out, dim=1)\n",
    "            batch_acc = (batch_pred == batch.answer).sum()\n",
    "            loss += float(batch_loss)\n",
    "            acc += batch_acc.data.item()\n",
    "            total += args.train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
